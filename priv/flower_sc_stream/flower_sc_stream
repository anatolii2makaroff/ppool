#!/usr/bin/python
#
# Flow scheduler worker
#

import sys
import time
import json
import sqlite3
import glob
from operator import mul
import math


# API

def read():
    line = sys.stdin.readline()
    return line.strip()


def log(m):
    sys.stderr.write("{}: {}\n".format(time.time(), m))
    sys.stderr.flush()


def send(m):
    sys.stdout.write("{}\n".format(m))
    sys.stdout.flush()

# Process - actor
#  read - recieve message from world
#  send - send message to world
#  log  -  logging anything


def main(p, node, t):
    """
        t - time interval in secs
    """
    tb = time.time()

    msg = read()

    log("get message: " + msg)

    while 1:

        #  set clester network balancer
        #  get main node floating ip
        log("get main node..")
        m_nodes = get_active_nodes(p)

        log("get active nodes: {}".format(m_nodes))

        if len(m_nodes) == 0:
            m_node = None
        else:
            m_node = m_nodes[0]

        if m_node != node:
            log("i'm not main node {} ..set rserv".format(m_node))
            time.sleep(int(t))
            send("ok")

            continue
        else:
            log("main node {} ..set admin".format(m_node))

        if time.time() - tb < int(t) * 5:
            time.sleep(int(t))
            send("ok")

            continue
        else:
            tb = time.time()

        #  get node's statistic
        log("get stats..")

        pstats = get_stats(p)

        #  gel flows scenes
        #  start/stop flows

        log("start init flows..")

        start_init_flows(p, pstats, m_nodes)

        pstats = get_stats_decrease(p)

        log("start decrease flows..")

        decrease_ppools(pstats, m_nodes)

        send("ok")
        log("tick..")

        time.sleep(int(t))


def connect(p):
    return sqlite3.connect(p + '/db/node_collector.db', timeout=3)


def decrease_ppools(pstats, m_nodes):

    """
    for f in get_flows(p):
        f_name, f_active, f_ppools, f_count, f_ram = f.get("name"), f.get("active"),\
            [[y.get("cmd").split("::")[3] for y in x.get("cook")
              if y.get("cmd").split("::")[2] == "start_pool"]
             for x in f.get("scenes")
             if x["name"] == f.get("start_scene")][0],\
            [[int(y.get("cmd").split("::")[4]) for y in x.get("cook")
              if y.get("cmd").split("::")[2] == "start_pool"]
             for x in f.get("scenes")
             if x["name"] == f.get("start_scene")][0],\
            [[int(y.get("cmd").split("::")[4].split(" ")[1][:-1]) for y in x.get("cook")
              if y.get("cmd").split("::")[2] == "start_all_workers"]
             for x in f.get("scenes")
             if x["name"] == f.get("start_scene")][0]
    """

    for k, v in pstats.iteritems():

        if k not in m_nodes:
            continue

        log("change ppool : {} {}".format(k, v))

        for i in v:
            if i[2] > 3:
                i[2] = 3
                log("too big : {} set {}".format(k, i[2]))

            if i[2] > 0:
                send("system::{}::change_limit::{}::{}".format(k, i[0], int(i[1] + i[2])))
                log("add ppool : {} {}".format(k, i[0]))
            else:
                send("system::{}::stop_all_workers::{}::{}".format(k, i[0], int(i[1] + i[2])))
                log("stop ppool : {} {}".format(k, i[0]))


def start_init_flows(p, pstats, m_nodes):

    for f in get_flows(p):
        f_name, f_active, f_ppools, f_count, f_ram = f.get("name"), f.get("active"),\
            [[y.get("cmd").split("::")[3] for y in x.get("cook")
              if y.get("cmd").split("::")[2] == "start_pool"]
             for x in f.get("scenes")
             if x["name"] == f.get("start_scene")][0],\
            [[int(y.get("cmd").split("::")[4]) for y in x.get("cook")
              if y.get("cmd").split("::")[2] == "start_pool"]
             for x in f.get("scenes")
             if x["name"] == f.get("start_scene")][0],\
            [[int(y.get("cmd").split("::")[4].split(" ")[1][:-1]) for y in x.get("cook")
              if y.get("cmd").split("::")[2] == "start_all_workers"]
             for x in f.get("scenes")
             if x["name"] == f.get("start_scene")][0]

        log("read flow {} {} {} {} {}".format(f_name, f_active, f_ppools, f_count, f_ram))

        # if flow not active stop on all nodes
        if f_active == 0:
            log("stop on all nodes..{}".format(f_name))
            send("system::all::{}::stop".format(f_name))

        elif f_active == 1:
            # check already started

            first_start = True
            for k, v in pstats.iteritems():

                if k not in m_nodes:
                    continue

                if set(f_ppools).issubset(set([x[1] for x in v])):

                    log("on node {} flow {} is started".format(k, f_name))
                    send("system::{}::{}::start".format(k, f_name))

                    first_start = False

                else:
                    log("on node {} flow {} is NOT started".format(k, f_name))

                    # force clear
                    send("system::{}::{}::stop".format(k, f_name))

                    first_start = False or first_start

            # need first start

            if first_start:
                log("flow {} first start {}".format(f_name, first_start))

                _ram_need = sum(map(mul, f_ram, f_count))

                # check resources for first start
                # ram cpu

                v = get_load_nodes(p)
                for n, l in v.iteritems():
                    log("load nodes {} {} {}".format(n, l, _ram_need))

                    if l[1] > _ram_need and l[0] < 85:
                        send("system::{}::{}::start".format(n, f_name))
                        break
                    else:
                        continue


def get_load_nodes(p):
    con = connect(p)
    cur = con.cursor()

    cur.execute(""" select * from(
                     select s.node, AVG(s.cpu_percent) cpu, (1 - AVG(s.ram_percent)/100) * s.ram_count ram
                         from node_stat s left join (select node, active, MAX(date) from node_list
                                                      where date > DATETIME('NOW', '-1 minutes')
                                                    ) l
                          on s.node = l.node
                         where s.date > DATETIME('NOW', '-1 minutes')
                            and ifnull(l.active, 1) = 1
                         group by s.node
                   ) t
                   order by t.ram desc, t.cpu asc
                """)

    data = {}

    for row in cur:
        data[row[0]] = (row[1], row[2])

    con.close()

    return data


def get_active_nodes(p):
    con = connect(p)
    cur = con.cursor()

    cur.execute("""select s.node, s.date
                         from node_stat s left join (select node, active, MAX(date) from node_list
                                                      where date > DATETIME('NOW', '-1 minutes')
                                                    ) l
                          on s.node = l.node
                         where s.date > DATETIME('NOW', '-1 minutes')
                            and ifnull(l.active, 1) = 1
                         group by s.node
                         order by s.date, s.node desc
                         -- LIMIT 1
                """)

    data = []

    for row in cur:
        data.append(row[0])

    con.close()

    return data


def get_stats_decrease(p):
    con = connect(p)
    cur = con.cursor()

    cur.execute("""

        select t.node, t.name, t.cnt,
               t.ok/(60*1000/t.time),
               t.ok, t.nm, t.timeo, t.err

          from (
             select s.node, l.name
                    ,MAX(ok) ok
                    ,MAX(running) run
                    ,SUM(l.nomore) nm
                    ,MAX(l.error) err
                    ,MAX(l.timeout) timeo
                    ,round(MAX(elapsed)/1000,2) time
                    ,MIN(s.count) cnt

                    from ppool_list l, (select s.node, s.name, MIN(s.count) count
                                              from ppool_stat s
                                               where s.date > DATETIME('NOW', '-1 minutes')
                                              group by s.node, s.name) s

                     where l.node = s.node
                         and l.name = s.name
                         and l.date > DATETIME('NOW', '-1 minutes')
                     group by s.node, s.name

               ) t


               """)

    data = {}
    for row in cur:

        if row[2] is None or row[2] == 0 or row[3] is None:
            continue

        workers = math.ceil(row[3]) - math.ceil(row[2])

        log("ppool stat {} need {}".format(row, workers))

        d = [row[1], math.ceil(row[2]), workers]

        # if nomore > 70% when min ppool to 1

        log("check nomore rate {} / {} / {} > 70".format(row[5], row[4], row[2]))
        if row[5] / (row[4] + 0.1) > 0.7:
            log("too many nomore {} add workers:{} ".format(row[1],
                                                            math.ceil(1.0*row[2]*row[5]/row[4])
                                                            ))

            d[2] = math.ceil(1.0*row[2]*row[5]/row[4])

        # if err > 70% when min ppool to 1

        log("check err rate {} / {} > 70".format(row[7], row[4]))
        if row[7] / (row[4] + 0.1) > 0.7:
            log("too many err {}: {} > 70% ".format(row[1],
                                                    row[7]/(row[4] + 0.1)))
            d[2] = -1 * math.ceil(row[2]) + 1
        else:
            if d[2] == 0:
                continue

        if data.get(row[0]) is None:
            data[row[0]] = [d]
        else:
            data.get(row[0]).append(d)

    con.close()

    return data


def get_stats(p):
    con = connect(p)
    cur = con.cursor()

    cur.execute("""

        select s.node, l.name
               ,MAX(ok)
               ,MAX(l.error), MAX(l.timeout), MAX(running), SUM(l.nomore)
               ,round(AVG(elapsed)/1000,2)

               ,AVG(s.count), AVG(s.cpu_percent), AVG(s.ram_percent)

               ,AVG(ns.cpu_percent), AVG(ns.ram_percent)
               ,ns.cpu_count, ns.ram_count

             from ppool_list l, ppool_stat s,
                  node_stat ns
             where l.node = s.node
                 and l.name = s.name
                 and s.node = ns.node
                 and l.date > DATETIME('NOW', '-1 minutes')
             group by s.node, s.name


               """)

    data = {}
    for row in cur:
        if data.get(row[0]) is None:
            data[row[0]] = [row]
        else:
            data[row[0]].append(row)

    con.close()

    return data


def get_flows(p):
    l = glob.glob(p + "/flows/*.json")

    data = []
    for i in l:
        with open(i) as f:
            data.append(json.loads(f.read()))

    return data


if __name__ == "__main__":
    main(sys.argv[1], sys.argv[2], sys.argv[3])
